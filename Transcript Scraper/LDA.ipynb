{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 747437 items in vocab_frame\n",
      "     words\n",
      "hi      hi\n",
      "im      im\n",
      "bob    bob\n",
      "ross  ross\n",
      "next  next\n",
      "Timing was: 51.78325128555298\n",
      "(403, 1194)\n",
      "[[  3.33066907e-16   6.57155087e-01   5.40618918e-01 ...,   8.05370901e-01\n",
      "    7.94339575e-01   7.69224081e-01]\n",
      " [  6.57155087e-01  -2.22044605e-16   5.93467262e-01 ...,   7.35639282e-01\n",
      "    6.70042039e-01   6.82503565e-01]\n",
      " [  5.40618918e-01   5.93467262e-01   0.00000000e+00 ...,   7.59900013e-01\n",
      "    7.49788280e-01   7.06075570e-01]\n",
      " ..., \n",
      " [  8.05370901e-01   7.35639282e-01   7.59900013e-01 ...,  -2.22044605e-16\n",
      "    6.44629773e-01   6.58404655e-01]\n",
      " [  7.94339575e-01   6.70042039e-01   7.49788280e-01 ...,   6.44629773e-01\n",
      "    0.00000000e+00   4.51341624e-01]\n",
      " [  7.69224081e-01   6.82503565e-01   7.06075570e-01 ...,   6.58404655e-01\n",
      "    4.51341624e-01  -4.44089210e-16]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt4Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "#dir = os.path.dirname(__file__)\n",
    "\n",
    "file_paths = [r'C:\\Users\\Danny\\PycharmProjects\\BobRoss\\Transcript Scraper\\Transcripts\\The Joy of Painting - Season 1\\Bob Ross - Autumn Mountain (Season 1 Episode 7)\\Bob Ross - Autumn Mountain (Season 1 Episode 7).txt',\n",
    "               r'C:\\Users\\Danny\\PycharmProjects\\BobRoss\\Transcript Scraper\\Transcripts\\The Joy of Painting - Season 1\\Bob Ross - A Walk in the Woods (Season 1 Episode 1)\\Bob Ross - A Walk in the Woods (Season 1 Episode 1).txt']\n",
    "file_paths = []\n",
    "transcripts = []\n",
    "titles = []\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords.extend(['little', 'coat', 'shoot','ol','sorta', 'odorless', 'em', 'wan', 'wan na', 'yeah', 'something'])\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# not super pythonic, no, not at all.\n",
    "# use extend so it's a big flat list of vocab\n",
    "pattern = re.compile('Bob Ross - (.*)\\.txt$')\n",
    "for root, dirs, files in os.walk(r\".\\Transcripts\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and file not in [\"data.txt\", \"paint.txt\"]:\n",
    "            f = os.path.join(root, file)\n",
    "            file_paths.append(f)\n",
    "            with open(f, 'rb') as infile:\n",
    "                text = infile.read().decode('utf8')\n",
    "                if text:\n",
    "                    transcripts.append(text)\n",
    "                    titles.append(pattern.match(file)[1])\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "#for file in file_paths:\n",
    " #   with open(file, 'rb') as infile:\n",
    "  #      text = infile.read().decode('utf8')\n",
    "   #     transcripts.append(text)\n",
    "\n",
    "for transcript in transcripts:\n",
    "    allwords_stemmed = tokenize_and_stem(transcript)  # for each item in transcripts, tokenize/stem\n",
    "    if allwords_stemmed:\n",
    "        totalvocab_stemmed.extend(allwords_stemmed)  # extend the 'totalvocab_stemmed' list\n",
    "    allwords_tokenized = tokenize_only(transcript)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "print(vocab_frame.head())\n",
    "\n",
    "# gets us a vocab for the entire corpus and then we need to create a Term Freq Inverse Document Frequency Matrx\n",
    "#       epi_1 -- epi_2\n",
    "#word1  x           y\n",
    "#word2  z           u\n",
    "\n",
    "# use the tfidf to do some clustering, maybe k means\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'vectorizer.pkl')\n",
    "tfidf_vectorizer = joblib.load('vectorizer.pkl')\n",
    "\n",
    "start = time.time()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(transcripts) #fit the vectorizer to synopses\n",
    "end = time.time()\n",
    "print(\"Timing was: \" + str(end-start))\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing was: 45.905484437942505\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#remove proper names - preprocess is a list of lists\n",
    "#%time preprocess = [strip_proppers_POS(doc) for doc in transcripts]\n",
    "\n",
    "#tokenize\n",
    "#print(preprocess[1])\n",
    "tokenized_text = []\n",
    "start = time.time()\n",
    "for text in transcripts:\n",
    "    tokenized_text.append(tokenize_and_stem(text))\n",
    "end = time.time()\n",
    "print(\"Timing was: \" + str(end-start))\n",
    "\n",
    "#remove stop words\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 4s\n"
     ]
    }
   ],
   "source": [
    "# run the LDA model (long wait)\n",
    "%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"snow\" + 0.014*\"winter\" + 0.013*\"ok\" + 0.011*\"scene\" + 0.011*\"alright\" + 0.008*\"mountain\" + 0.008*\"caus\" + 0.007*\"pprusian\" + 0.007*\"ole\" + 0.007*\"phphphthalo\"'),\n",
       " (1,\n",
       "  '0.008*\"alright\" + 0.007*\"bush\" + 0.006*\"gesso\" + 0.006*\"caus\" + 0.006*\"doesnt\" + 0.006*\"corner\" + 0.006*\"though\" + 0.005*\"inch\" + 0.005*\"ochr\" + 0.005*\"decid\"'),\n",
       " (2,\n",
       "  '0.011*\"lay\" + 0.009*\"sap\" + 0.008*\"tini\" + 0.008*\"play\" + 0.008*\"bush\" + 0.007*\"cloud\" + 0.007*\"umber\" + 0.007*\"music\" + 0.007*\"ok\" + 0.007*\"shadow\"'),\n",
       " (3,\n",
       "  '0.020*\"mountain\" + 0.010*\"cloud\" + 0.008*\"shadow\" + 0.007*\"bush\" + 0.007*\"reflect\" + 0.007*\"layer\" + 0.006*\"roll\" + 0.006*\"decis\" + 0.006*\"inch\" + 0.005*\"stroke\"'),\n",
       " (4,\n",
       "  '0.035*\"barn\" + 0.033*\"build\" + 0.031*\"roof\" + 0.022*\"hous\" + 0.022*\"door\" + 0.018*\"window\" + 0.016*\"board\" + 0.016*\"fenc\" + 0.012*\"cabin\" + 0.012*\"bare\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'winter', 'ok', 'scene', 'alright', 'mountain', 'caus', 'pprusian', 'ole', 'phphphthalo', 'cold', 'twoinch', 'decid', 'doesnt', 'shadow', 'upward', 'somethin', 'care', 'cloud', 'wash']\n",
      "\n",
      "['alright', 'bush', 'gesso', 'caus', 'doesnt', 'corner', 'though', 'inch', 'ochr', 'decid', 'sienna', 'liner', 'phphphthalo', 'rock', 'sap', 'clear', 'least', 'stone', 'wave', 'hang']\n",
      "\n",
      "['lay', 'sap', 'tini', 'play', 'bush', 'cloud', 'umber', 'music', 'ok', 'shadow', 'perman', 'next', 'bare', 'burnt', 'limb', 'scrape', 'reflect', 'almighti', 'mountain', 'week']\n",
      "\n",
      "['mountain', 'cloud', 'shadow', 'bush', 'reflect', 'layer', 'roll', 'decis', 'inch', 'stroke', 'lay', 'corner', 'tini', 'pressur', 'base', 'bare', 'angl', 'hes', 'least', 'illus']\n",
      "\n",
      "['barn', 'build', 'roof', 'hous', 'door', 'window', 'board', 'fenc', 'cabin', 'bare', 'path', 'post', 'shingl', 'wood', 'shed', 'stand', 'farmer', 'rail', 'cow', 'porch']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "matrix = [ele[1] for ele in topics_matrix]\n",
    "topics_matrix = np.array(matrix)\n",
    "\n",
    "topic_words = topics_matrix[:,:,0]\n",
    "for i in topic_words:\n",
    "    print([str(word) for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(topics_matrix[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}